#!/usr/bin/env python3
"""
è°ƒè¯•è„šæœ¬ - æŸ¥çœ‹çŸ¥ä¹çƒ­æ¦œé¡µé¢çš„å®é™…HTMLç»“æ„
"""
import sys
import os
import requests
from bs4 import BeautifulSoup
import json
import re

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import SPIDER_CONFIG

def debug_zhihu_page():
    """è°ƒè¯•çŸ¥ä¹çƒ­æ¦œé¡µé¢ç»“æ„"""
    print("ğŸ” è°ƒè¯•çŸ¥ä¹çƒ­æ¦œé¡µé¢ç»“æ„...")
    
    try:
        # åˆ›å»ºä¼šè¯
        session = requests.Session()
        headers = SPIDER_CONFIG['headers'].copy()
        session.headers.update(headers)
        
        # è·å–é¡µé¢
        print("æ­£åœ¨è·å–çŸ¥ä¹çƒ­æ¦œé¡µé¢...")
        response = session.get(SPIDER_CONFIG['zhihu_hot_url'], timeout=30)
        response.raise_for_status()
        
        print(f"âœ… é¡µé¢è·å–æˆåŠŸï¼ŒçŠ¶æ€ç : {response.status_code}")
        print(f"é¡µé¢å¤§å°: {len(response.text)} å­—ç¬¦")
        
        # è§£æHTML
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ä¿å­˜é¡µé¢å†…å®¹åˆ°æ–‡ä»¶
        with open('zhihu_hot_debug.html', 'w', encoding='utf-8') as f:
            f.write(response.text)
        print("âœ… é¡µé¢å†…å®¹å·²ä¿å­˜åˆ° zhihu_hot_debug.html")
        
        # æŸ¥æ‰¾å¯èƒ½çš„å®¹å™¨
        print("\nğŸ” æŸ¥æ‰¾å¯èƒ½çš„çƒ­æ¦œå®¹å™¨...")
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰divå…ƒç´ çš„classå±æ€§
        divs_with_class = soup.find_all('div', class_=True)
        class_names = set()
        for div in divs_with_class:
            classes = div.get('class', [])
            for cls in classes:
                if any(keyword in cls.lower() for keyword in ['hot', 'list', 'item', 'question']):
                    class_names.add(cls)
        
        if class_names:
            print("æ‰¾åˆ°å¯èƒ½ç›¸å…³çš„CSSç±»å:")
            for cls in sorted(class_names):
                print(f"  - {cls}")
        
        # 2. æŸ¥æ‰¾æ‰€æœ‰é—®é¢˜é“¾æ¥
        print("\nğŸ” æŸ¥æ‰¾çŸ¥ä¹é—®é¢˜é“¾æ¥...")
        links = soup.find_all('a', href=True)
        question_links = []
        
        for link in links:
            href = link.get('href', '')
            if '/question/' in href:
                title = link.get_text(strip=True)
                if title and len(title) > 5:  # è¿‡æ»¤çŸ­æ ‡é¢˜
                    question_links.append({
                        'href': href,
                        'title': title[:50] + '...' if len(title) > 50 else title
                    })
        
        print(f"æ‰¾åˆ° {len(question_links)} ä¸ªé—®é¢˜é“¾æ¥:")
        for i, link in enumerate(question_links[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  {i}. {link['title']}")
            print(f"     {link['href']}")
        
        if len(question_links) > 10:
            print(f"  ... è¿˜æœ‰ {len(question_links) - 10} ä¸ªé“¾æ¥")
        
        # 3. æŸ¥æ‰¾è„šæœ¬æ ‡ç­¾ä¸­çš„æ•°æ®
        print("\nğŸ” æŸ¥æ‰¾JavaScriptæ•°æ®...")
        script_tags = soup.find_all('script')
        
        data_patterns = [
            'hotList', 'hot_list', 'topstory', 'initialState', 
            'INITIAL_STATE', 'window.__', 'data'
        ]
        
        found_data = False
        for i, script in enumerate(script_tags):
            if script.string:
                content = script.string[:200]  # åªçœ‹å‰200å­—ç¬¦
                for pattern in data_patterns:
                    if pattern in script.string:
                        print(f"è„šæœ¬ {i+1} åŒ…å« '{pattern}': {content}...")
                        found_data = True
                        break
                if found_data:
                    break
        
        if not found_data:
            print("æœªåœ¨JavaScriptä¸­æ‰¾åˆ°æ˜æ˜¾çš„æ•°æ®æ¨¡å¼")
        
        # 4. æŸ¥æ‰¾ç‰¹å®šçš„ç»“æ„æ¨¡å¼
        print("\nğŸ” æŸ¥æ‰¾ç‰¹å®šçš„HTMLç»“æ„æ¨¡å¼...")
        
        # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„åˆ—è¡¨ç»“æ„
        list_elements = []
        
        # æŸ¥æ‰¾ul/olåˆ—è¡¨
        lists = soup.find_all(['ul', 'ol'])
        for lst in lists:
            items = lst.find_all('li')
            if len(items) > 5:  # å¯èƒ½æ˜¯çƒ­æ¦œåˆ—è¡¨
                list_elements.append(f"{'ul' if lst.name == 'ul' else 'ol'} with {len(items)} li items")
        
        # æŸ¥æ‰¾sectionå®¹å™¨
        sections = soup.find_all('section')
        for section in sections:
            articles = section.find_all('article')
            if len(articles) > 5:
                list_elements.append(f"section with {len(articles)} article items")
        
        if list_elements:
            print("æ‰¾åˆ°å¯èƒ½çš„åˆ—è¡¨ç»“æ„:")
            for elem in list_elements:
                print(f"  - {elem}")
        else:
            print("æœªæ‰¾åˆ°æ˜æ˜¾çš„åˆ—è¡¨ç»“æ„")
        
        return len(question_links) > 0
        
    except Exception as e:
        print(f"âŒ è°ƒè¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    print("ğŸ› ï¸  çŸ¥ä¹çƒ­æ¦œé¡µé¢ç»“æ„è°ƒè¯•")
    print("=" * 50)
    
    success = debug_zhihu_page()
    
    if success:
        print(f"\nâœ… è°ƒè¯•å®Œæˆï¼è¯·æ£€æŸ¥ç”Ÿæˆçš„ zhihu_hot_debug.html æ–‡ä»¶")
        print("å¯ä»¥æ®æ­¤ä¼˜åŒ–çˆ¬è™«çš„HTMLè§£æé€»è¾‘")
    else:
        print(f"\nâŒ è°ƒè¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")

if __name__ == '__main__':
    main()
