"""
ä¸»ç¨‹åºå…¥å£ - çŸ¥ä¹çƒ­æ¦œçˆ¬è™«
"""
import logging
import argparse
import sys
from datetime import datetime, timedelta
from typing import Optional

from utils import setup_logging, print_banner, save_to_json, format_timestamp
from scraper import ZhihuSpider
from processor import DataProcessor
from database import db_manager

logger = logging.getLogger(__name__)

class ZhihuHotSpider:
    """çŸ¥ä¹çƒ­æ¦œçˆ¬è™«ä¸»ç±»"""
    
    def __init__(self):
        self.spider = None
        self.processor = DataProcessor()
        
    def setup(self):
        """åˆå§‹åŒ–è®¾ç½®"""
        try:
            # è®¾ç½®æ—¥å¿—
            setup_logging()
            logger.info("ç¨‹åºå¯åŠ¨")
            
            # åˆå§‹åŒ–çˆ¬è™«
            self.spider = ZhihuSpider()
            
            # åˆ›å»ºæ•°æ®åº“è¡¨
            db_manager.create_tables()
            
            logger.info("åˆå§‹åŒ–å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def run_once(self, save_json: bool = False) -> bool:
        """
        æ‰§è¡Œä¸€æ¬¡çˆ¬å–
        
        Args:
            save_json: æ˜¯å¦ä¿å­˜ä¸ºJSONæ–‡ä»¶
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info("å¼€å§‹æ‰§è¡Œçˆ¬å–ä»»åŠ¡")
            
            # è·å–çƒ­æ¦œæ•°æ®
            raw_data = self.spider.fetch_hot_list()
            if not raw_data:
                logger.warning("æœªè·å–åˆ°çƒ­æ¦œæ•°æ®")
                return False
            
            # æ•°æ®å¤„ç†
            processed_data = self.processor.process_hot_items(raw_data)
            if not processed_data:
                logger.warning("æ•°æ®å¤„ç†åæ— æœ‰æ•ˆæ•°æ®")
                return False
            
            # å»é‡å’Œæ’åº
            unique_data = self.processor.deduplicate_items(processed_data)
            # sorted_data = self.processor.sort_by_hot_index(unique_data)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            saved_count = db_manager.save_hot_items(unique_data)
            logger.info(f"æˆåŠŸä¿å­˜ {saved_count} æ¡æ•°æ®åˆ°æ•°æ®åº“")
            
            # å¯é€‰ï¼šä¿å­˜ä¸ºJSONæ–‡ä»¶
            if save_json:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                json_filename = f"zhihu_hot_{timestamp}.json"
                save_to_json(unique_data, json_filename)
            
            # ç”Ÿæˆå¹¶æ˜¾ç¤ºæ‘˜è¦
            summary = self.processor.generate_summary(unique_data)
            self._print_summary(summary, unique_data[:5])  # æ˜¾ç¤ºå‰5æ¡
            
            logger.info("çˆ¬å–ä»»åŠ¡æ‰§è¡Œå®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"æ‰§è¡Œçˆ¬å–ä»»åŠ¡å¤±è´¥: {e}")
            return False
    
    def run_scheduled(self, interval: int = 3600):
        """
        å®šæ—¶æ‰§è¡Œçˆ¬å–
        
        Args:
            interval: é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰
        """
        import time
        
        logger.info(f"å¼€å§‹å®šæ—¶çˆ¬å–ï¼Œé—´éš” {interval} ç§’")
        
        try:
            while True:
                success = self.run_once()
                if success:
                    logger.info(f"ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´: {format_timestamp(datetime.now().replace(second=0, microsecond=0).replace(minute=0) + timedelta(hours=1))}")
                else:
                    logger.warning("æœ¬æ¬¡çˆ¬å–å¤±è´¥")
                
                time.sleep(interval)
                
        except KeyboardInterrupt:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œç¨‹åºé€€å‡º")
        except Exception as e:
            logger.error(f"å®šæ—¶çˆ¬å–å¼‚å¸¸: {e}")
    
    def cleanup_old_data(self, days: int = 7):
        """
        æ¸…ç†æ—§æ•°æ®
        
        Args:
            days: ä¿ç•™å¤©æ•°
        """
        try:
            deleted_count = db_manager.clear_old_data(days)
            logger.info(f"æ¸…ç†äº† {deleted_count} æ¡è¶…è¿‡ {days} å¤©çš„æ—§æ•°æ®")
        except Exception as e:
            logger.error(f"æ¸…ç†æ—§æ•°æ®å¤±è´¥: {e}")
    
    def show_recent_data(self, limit: int = 20):
        """
        æ˜¾ç¤ºæœ€è¿‘çš„æ•°æ®
        
        Args:
            limit: æ˜¾ç¤ºæ¡æ•°
        """
        try:
            items = db_manager.get_hot_items(limit)
            if not items:
                print("æ•°æ®åº“ä¸­æš‚æ— æ•°æ®")
                return
            
            print(f"\næœ€è¿‘ {len(items)} æ¡çƒ­æ¦œæ•°æ®:")
            print("-" * 80)
            
            for i, item in enumerate(items, 1):
                print(f"{i:2d}. {item.title}")
                print(f"    çƒ­åº¦: {item.hot_index} | å›ç­”: {item.answer_count} | å…³æ³¨: {item.follower_count}")
                print(f"    æ—¶é—´: {format_timestamp(item.created_time)} | URL: {item.url}")
                print()
                
        except Exception as e:
            logger.error(f"æ˜¾ç¤ºæ•°æ®å¤±è´¥: {e}")
    
    def _print_summary(self, summary: dict, top_items: list):
        """æ‰“å°æ‘˜è¦ä¿¡æ¯"""
        print("\n" + "="*50)
        print("ğŸ“Š çˆ¬å–æ‘˜è¦")
        print("="*50)
        print(f"æ€»æ¡æ•°: {summary['total_count']}")
        print(f"å¹³å‡çƒ­åº¦: {summary['avg_hot_index']:.2f}")
        print(f"æœ€é«˜çƒ­åº¦: {summary['max_hot_index']:.2f}")
        print(f"æœ€ä½çƒ­åº¦: {summary['min_hot_index']:.2f}")
        print(f"çˆ¬å–æ—¶é—´: {summary['timestamp']}")
        
        if top_items:
            print("\nğŸ”¥ çƒ­åº¦æ’è¡Œæ¦œ TOP 5:")
            print("-" * 50)
            for i, item in enumerate(top_items, 1):
                print(f"{i}. {item['title'][:30]}...")
                print(f"   çƒ­åº¦: {item['hot_index']} | å›ç­”: {item['answer_count']}")
        
        print("="*50)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        if self.spider:
            self.spider.close()
        logger.info("èµ„æºæ¸…ç†å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='çŸ¥ä¹çƒ­æ¦œçˆ¬è™«ç¨‹åº')
    parser.add_argument('--mode', choices=['once', 'schedule', 'show', 'cleanup'], 
                       default='once', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--interval', type=int, default=3600, 
                       help='å®šæ—¶æ¨¡å¼çš„é—´éš”æ—¶é—´ï¼ˆç§’ï¼‰')
    parser.add_argument('--json', action='store_true', 
                       help='æ˜¯å¦ä¿å­˜ä¸ºJSONæ–‡ä»¶')
    parser.add_argument('--limit', type=int, default=20, 
                       help='æ˜¾ç¤ºæ•°æ®çš„æ¡æ•°')
    parser.add_argument('--days', type=int, default=7, 
                       help='æ¸…ç†è¶…è¿‡æŒ‡å®šå¤©æ•°çš„æ—§æ•°æ®')
    
    args = parser.parse_args()
    
    # æ‰“å°æ¨ªå¹…
    print_banner()
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    spider_app = ZhihuHotSpider()
    
    try:
        # åˆå§‹åŒ–
        if not spider_app.setup():
            sys.exit(1)
        
        # æ ¹æ®æ¨¡å¼æ‰§è¡Œ
        if args.mode == 'once':
            success = spider_app.run_once(save_json=args.json)
            sys.exit(0 if success else 1)
            
        elif args.mode == 'schedule':
            spider_app.run_scheduled(interval=args.interval)
            
        elif args.mode == 'show':
            spider_app.show_recent_data(limit=args.limit)
            
        elif args.mode == 'cleanup':
            spider_app.cleanup_old_data(days=args.days)
            
    except KeyboardInterrupt:
        logger.info("ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        sys.exit(1)
    finally:
        spider_app.cleanup()

if __name__ == '__main__':
    main()
