#!/usr/bin/env python3
"""
ç®€å•æµ‹è¯•è„šæœ¬ - æµ‹è¯•çŸ¥ä¹çƒ­æ¦œçˆ¬è™«æ˜¯å¦èƒ½æ­£å¸¸å·¥ä½œ
"""
import sys
import os
import logging

# æ·»åŠ å½“å‰ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_scraper_only():
    """ä»…æµ‹è¯•çˆ¬è™«åŠŸèƒ½ï¼Œä¸æ¶‰åŠæ•°æ®åº“"""
    print("ğŸ•·ï¸  æµ‹è¯•çŸ¥ä¹çƒ­æ¦œçˆ¬è™«...")
    
    try:
        # ç®€å•çš„æ—¥å¿—è®¾ç½®
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        
        from scraper import ZhihuSpider
        
        # åˆ›å»ºçˆ¬è™«å®ä¾‹
        spider = ZhihuSpider()
        
        # è·å–çƒ­æ¦œæ•°æ®
        print("æ­£åœ¨è·å–çƒ­æ¦œæ•°æ®...")
        raw_data = spider.fetch_hot_list()
        
        if not raw_data:
            print("âŒ æœªèƒ½è·å–åˆ°çƒ­æ¦œæ•°æ®")
            return False
        
        print(f"âœ… æˆåŠŸè·å– {len(raw_data)} æ¡åŸå§‹æ•°æ®")
        
        # æ˜¾ç¤ºå‰5æ¡æ•°æ®
        print("\nğŸ“Š çƒ­æ¦œå‰5æ¡:")
        print("-" * 60)
        
        for i, item in enumerate(raw_data[:5], 1):
            print(f"{i}. {item.get('title', 'N/A')}")
            print(f"   é—®é¢˜ID: {item.get('question_id', 'N/A')}")
            print(f"   çƒ­åº¦: {item.get('hot_index', 'N/A')}")
            print(f"   URL: {item.get('url', 'N/A')}")
            print()
        
        # æ¸…ç†èµ„æº
        spider.close()
        
        print("âœ… çˆ¬è™«æµ‹è¯•æˆåŠŸå®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_with_processing():
    """æµ‹è¯•çˆ¬è™«å’Œæ•°æ®å¤„ç†"""
    print("ğŸ”„ æµ‹è¯•æ•°æ®å¤„ç†...")
    
    try:
        from processor import DataProcessor
        from scraper import ZhihuSpider
        
        logging.basicConfig(level=logging.WARNING)  # å‡å°‘æ—¥å¿—è¾“å‡º
        
        spider = ZhihuSpider()
        processor = DataProcessor()
        
        # è·å–æ•°æ®
        raw_data = spider.fetch_hot_list()
        if not raw_data:
            print("âŒ æ— æ³•è·å–æ•°æ®")
            return False
        
        # å¤„ç†æ•°æ®
        processed_data = processor.process_hot_items(raw_data)
        if not processed_data:
            print("âŒ æ•°æ®å¤„ç†å¤±è´¥")
            return False
        
        print(f"âœ… æˆåŠŸå¤„ç† {len(processed_data)} æ¡æ•°æ®")
        
        # å»é‡å’Œæ’åº
        unique_data = processor.deduplicate_items(processed_data)
        sorted_data = processor.sort_by_hot_index(unique_data)
        
        print(f"âœ… å»é‡åå‰©ä½™ {len(unique_data)} æ¡æ•°æ®")
        
        # ç”Ÿæˆæ‘˜è¦
        summary = processor.generate_summary(sorted_data)
        print(f"âœ… æ•°æ®æ‘˜è¦: å¹³å‡çƒ­åº¦ {summary['avg_hot_index']:.2f}")
        
        spider.close()
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§ª çŸ¥ä¹çƒ­æ¦œçˆ¬è™«æµ‹è¯•")
    print("=" * 40)
    
    # å…ˆæµ‹è¯•åŸºæœ¬çˆ¬è™«åŠŸèƒ½
    success1 = test_scraper_only()
    
    if success1:
        print("\n" + "="*40)
        # å†æµ‹è¯•æ•°æ®å¤„ç†
        success2 = test_with_processing()
        
        if success2:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ç°åœ¨å¯ä»¥è¿è¡Œå®Œæ•´çš„çˆ¬è™«ç¨‹åºäº†ã€‚")
            print("è¿è¡Œå‘½ä»¤: python main.py --mode once")
        else:
            print("\nâš ï¸  çˆ¬è™«å·¥ä½œæ­£å¸¸ï¼Œä½†æ•°æ®å¤„ç†æœ‰é—®é¢˜ã€‚")
    else:
        print("\nğŸ’¡ å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚")
        print("å»ºè®®:")
        print("1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("2. æ£€æŸ¥æ˜¯å¦èƒ½è®¿é—® https://www.zhihu.com/hot")
        print("3. å°è¯•ä½¿ç”¨ä»£ç†æˆ–VPN")
    
    return 0 if success1 else 1

if __name__ == '__main__':
    sys.exit(main())
